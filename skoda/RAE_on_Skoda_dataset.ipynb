{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Skoda Dataset\n",
    "\n",
    "This dataset describes the activities of assembly-line workers in a car production environment. They consider the recognition of 11 activity classes performed in one of the quality assurance checkpoints of the\n",
    "production plan. In their study, one subject wore 19 3D accelerometers on both arms and perform a set of experiments using sensors placed on the two arms of a tester (10 sensors on the right arm and 9 sensor on the left arm). The original sample rate of this dataset was 98 Hz, but it was decimated to 30 Hz for comparison purposes with the other two datasets. The Skoda dataset has been employed to evaluate deep learning techniques in sensor networks, which makes it a proper dataset to evaluate our proposed Replacement AutoEncoder framework.\n",
    "\n",
    "More details:  http://www.ife.ee.ethz.ch/research/activity-recognition-datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 1: sm1_creating_sections.py\n",
    "Skoda contain two time-series, one collected from **left** hand of data subject and another from **right** hand. Here, we build training and test datasets from time-series. \n",
    "\n",
    "Look at **Experimental Settings** section of the paper for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Shape of train data =  (61945, 54, 30)\n",
      "Info: Shape of test data =  (15487, 54, 30)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created in September 2017\n",
    "\n",
    "This module merges seperated files in OPPORTUNITY Activity Recognition Data Set.\n",
    "It enables to create customized training and testing dataset. \n",
    "\n",
    "@author: mmalekzadeh\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "    \n",
    "def create_sections(hand):\n",
    "    if(hand == \"left\"):\n",
    "        all_d = np.loadtxt(\"left_hand.txt\", delimiter=',')\n",
    "    else:\n",
    "        all_d = np.loadtxt(\"right_hand.txt\", delimiter=',')\n",
    "        \n",
    "    ## The original sample rate of this dataset was 98 Hz,\n",
    "    ## but it was decimated to 30 Hz \n",
    "    ## for comparison purposes with the OPPORTUNITY dataset \\cite{ordonez2016deep}\n",
    "    all_d = all_d[::3]\n",
    "    \n",
    "    ## Sensory Data (10 sensors , each 6 values)(ecah column shows a recored)\n",
    "    all_data = all_d[:,1:71]\n",
    "    ## label for each entry (each recored has a label)\n",
    "    all_labels = all_d[:,0]\n",
    "\n",
    "\n",
    "    ## removing sensor ids\n",
    "    for i in range(0,60,6):\n",
    "        all_data = np.delete(all_data, i, 1)\n",
    "    \n",
    "    ##### ATTENTION #####\n",
    "    ### Just For Left Hand: Remove data of Sensor numer 30\n",
    "    if(hand == \"left\"):\n",
    "        seg1 = all_data[:,0:48]\n",
    "        seg2 = all_data[:,54:60]\n",
    "        all_data = np.append(seg1, seg2 ,axis=1)     \n",
    "    \n",
    "    all_data = all_data.T\n",
    "    all_labels = all_labels.T\n",
    "    \n",
    "    ##  This Variable Defines the Size of Sliding Window\n",
    "    ##  ( e.g. 100 means in each snapshot we just consider 100 consecutive observations of each sensor) \n",
    "    sliding_window_size = 30\n",
    "    \n",
    "    ##  Here We Choose Step Size for Building Diffrent Snapshots from Time-Series Data\n",
    "    ##  ( smaller step size will increase the amount of the instances and higher computational cost may be incurred )\n",
    "    step_size_of_sliding_window = 3\n",
    "        \n",
    "    size_features = all_data.shape[0]\n",
    "    size_all_data = all_data.shape[1]\n",
    "        \n",
    "    number_of_shots = round(((size_all_data - sliding_window_size)/step_size_of_sliding_window))\n",
    "        \n",
    "    ##  Create a 3D matrix for Storing Snapshots  \n",
    "    all_data_shots= np.zeros((number_of_shots , size_features , sliding_window_size ))\n",
    "    all_labels_shots = np.zeros(number_of_shots)\n",
    "    \n",
    "    for i in range(0 ,(size_all_data) - sliding_window_size  , step_size_of_sliding_window):\n",
    "        all_data_shots[i // step_size_of_sliding_window ] = all_data[0:size_features, i:i+sliding_window_size]\n",
    "        all_labels_shots[i // step_size_of_sliding_window] = np.argmax(np.bincount(all_labels[i:i+sliding_window_size].astype(int)))\n",
    "        \n",
    "    \"\"\"\n",
    "        1) above code steps on time-series data and extract temporal snapshots.\n",
    "        We build this new kind of dataset because its more flexible for Convolutional Neural Networks\n",
    "    \"\"\"\n",
    "    \n",
    "    ## There are 11 Gestures classes in dataset\n",
    "    ## Respectively :\n",
    "    ##    0 null class\n",
    "    ##    1 write on notepad\n",
    "    ##    2 open hood\n",
    "    ##    3 close hood\n",
    "    ##    4 check gaps on the front door\n",
    "    ##    5 open left front door\n",
    "    ##    6 close left front door\n",
    "    ##    7 close both left door\n",
    "    ##    8 check trunk gaps\n",
    "    ##    9 open and close trunk\n",
    "    ##    10 check steering wheel\n",
    "    \n",
    "    classes = np.array([32, 48, 49, 50,\n",
    "                        51, 52, 53, 54,\n",
    "                        55, 56, 57])\n",
    "    \n",
    "    ordinal_labels = np.zeros((all_labels_shots.shape[0]))\n",
    "    \n",
    "    for i in range(all_labels_shots.shape[0]):\n",
    "        k = np.where(classes == all_labels_shots[i])\n",
    "        ordinal_labels[i]= k[0]\n",
    "    \n",
    "    ## split into 80% for train and 20% for test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_data_shots,\n",
    "                                                        ordinal_labels,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=8)\n",
    "    \n",
    "    # ## normalize each sensorâ€™s data to have a zero mean and unity standard deviation.\n",
    "    for sid in range(X_train.shape[1]):\n",
    "        mean = (np.mean(X_train[:,sid,:]))\n",
    "        std  = (np.std(X_train[:,sid,:]))\n",
    "        X_train[:,sid,:] -= mean\n",
    "        X_test[:,sid,:] -= mean\n",
    "        X_train[:,sid,:] /= std\n",
    "        X_test[:,sid,:] /= std\n",
    "\n",
    "    \"\"\"\n",
    "    above code maps each class label to a number between 0 and 10\n",
    "    (This will be useful when you want to apply one-hot encoding using np_utils (from (keras.utils)) )\n",
    "    \"\"\"\n",
    "        #### Saving Datasets ###\n",
    "    np.save(\"train_data.npy\", X_train)\n",
    "    np.save(\"train_labels.npy\", y_train)\n",
    "    print(\"Info: Shape of train data = \",X_train.shape)\n",
    "    np.save(\"test_data.npy\", X_test)\n",
    "    np.save(\"test_labels.npy\", y_test)\n",
    "    print(\"Info: Shape of test data = \",X_test.shape)\n",
    "    return\n",
    "\n",
    "##########\n",
    "# Select Hand: \"left\" or \"right\"\n",
    "create_sections(hand = \"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 2: sm2_server_cnn.py\n",
    "This module trains a ConvNet model for using as the Server model.\n",
    "Look at **Experimental Settings** section of the paper for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61945/61945 [==============================] - 96s 2ms/step - loss: 0.7236 - acc: 0.7576\n",
      "Epoch 2/10\n",
      "61945/61945 [==============================] - 96s 2ms/step - loss: 0.3893 - acc: 0.8709\n",
      "Epoch 3/10\n",
      "61945/61945 [==============================] - 96s 2ms/step - loss: 0.3195 - acc: 0.8948\n",
      "Epoch 4/10\n",
      "61945/61945 [==============================] - 96s 2ms/step - loss: 0.2791 - acc: 0.9084\n",
      "Epoch 5/10\n",
      "61945/61945 [==============================] - 96s 2ms/step - loss: 0.2583 - acc: 0.9161\n",
      "Epoch 6/10\n",
      "61945/61945 [==============================] - 96s 2ms/step - loss: 0.2431 - acc: 0.9201\n",
      "Epoch 7/10\n",
      "61945/61945 [==============================] - 96s 2ms/step - loss: 0.2309 - acc: 0.9252\n",
      "Epoch 8/10\n",
      "61945/61945 [==============================] - 96s 2ms/step - loss: 0.2220 - acc: 0.9267\n",
      "Epoch 9/10\n",
      "61945/61945 [==============================] - 95s 2ms/step - loss: 0.2120 - acc: 0.9301\n",
      "Epoch 10/10\n",
      "61945/61945 [==============================] - 96s 2ms/step - loss: 0.2020 - acc: 0.9343\n",
      "15487/15487 [==============================] - 8s 487us/step\n",
      "\n",
      " ~~~ Result: \n",
      "acc: 95.08%\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created in  September  2017\n",
    "@author: mmalekzadeh\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from keras.models import Model # basic class for specifying and training a neural network\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from keras.models import model_from_json\n",
    "#####################################\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(2017)\n",
    "#####################################\n",
    "\n",
    "#### Global Variables ###\n",
    "train_data = np.load(\"train_data.npy\")\n",
    "test_data = np.load(\"test_data.npy\")\n",
    "train_label = np.load(\"train_labels.npy\")\n",
    "test_label = np.load(\"test_labels.npy\")\n",
    "\n",
    "batch_size = 64\n",
    "num_of_epochs = 10 ## CHOOSE NUMBER OF EPOCHES HERE\n",
    "kernel_size_1 = 5\n",
    "kernel_size_2 = 3\n",
    "\n",
    "pool_size_1 = 2\n",
    "pool_size_2 = 3  \n",
    "\n",
    "conv_depth_1 = 50 \n",
    "conv_depth_2 = 40 \n",
    "conv_depth_3 = 20 \n",
    "\n",
    "drop_prob_1 = 0.4 \n",
    "\n",
    "drop_prob_2 = 0.6 \n",
    "\n",
    "hidden_size = 400 \n",
    "\n",
    "num_classes = np.unique(train_label).shape[0] \n",
    "y_train = np_utils.to_categorical(train_label, num_classes) \n",
    "y_test = np_utils.to_categorical(test_label, num_classes) \n",
    "\n",
    "num_train, height, width = train_data.shape\n",
    "num_test = test_data.shape[0] \n",
    "\n",
    "inp = Input(shape=(height, width,1)) \n",
    "\n",
    "conv_0 = Convolution2D(conv_depth_1, (1 , kernel_size_1), padding='valid', activation='relu')(inp)\n",
    "conv_1 = Convolution2D(conv_depth_1, (1 , kernel_size_2), padding='same', activation='relu')(conv_0)\n",
    "dense_1 = Dense(conv_depth_1, activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(1, pool_size_1))(dense_1)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "\n",
    "conv_2 = Convolution2D(conv_depth_2, (1 , kernel_size_1), padding='valid', activation='relu')(drop_1)\n",
    "dense_2 = Dense(conv_depth_2, activation='relu')(conv_2)\n",
    "pool_2 = MaxPooling2D(pool_size=(1, pool_size_2))(dense_2)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "\n",
    "conv_3 = Convolution2D(conv_depth_3, (1 , kernel_size_2), padding='valid', activation='relu')(drop_2)\n",
    "drop_3 = Dropout(drop_prob_1)(conv_3)\n",
    "\n",
    "flat = Flatten()(drop_3)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "drop_4 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_4)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) \n",
    "\n",
    "train_data = np.expand_dims(train_data,axis=3)\n",
    "test_data = np.expand_dims(test_data,axis=3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "                       \n",
    "model.fit(train_data, y_train,                \n",
    "          batch_size = batch_size,\n",
    "          epochs = num_of_epochs,\n",
    "          verbose=1,\n",
    "          ) \n",
    "          \n",
    "scores = model.evaluate(test_data, y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
    "print(\"\\n ~~~ Result: \\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "#serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"server_cnn_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"server_cnn_model_weights.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 3: sm3_build_wbg_datasets.py\n",
    "Here we create three subsets from dataset: White, Gray, and Black.\n",
    "These datasets are used for training RAE.\n",
    "Look at **Experimental Settings** section of the paper for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: inference #0 is gray-listed\n",
      "Info: inference #1 is black-listed\n",
      "Info: inference #2 is gray-listed\n",
      "Info: inference #3 is gray-listed\n",
      "Info: inference #4 is white-listed\n",
      "Info: inference #5 is black-listed\n",
      "Info: inference #6 is black-listed\n",
      "Info: inference #7 is black-listed\n",
      "Info: inference #8 is white-listed\n",
      "Info: inference #9 is white-listed\n",
      "Info: inference #10 is white-listed\n",
      "Info: Size of white-listed training data is 19397\n",
      "Info: Size of black-listed training data is 15775\n",
      "Info: Size of gray-listed training data is 26773\n",
      "Info: Size of white-listed testing data is 4779\n",
      "Info: Size of black-listed testing data is 3923\n",
      "Info: Size of wgray-listed testing data is 6785\n",
      "Info: all lists are saved\n",
      "Info: lists of inferences are saved\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created in September  2017\n",
    "@author: mmalekzadeh\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "### Global Variables ###\n",
    "train_data = np.load(\"train_data.npy\")\n",
    "test_data = np.load(\"test_data.npy\")\n",
    "test_label = np.load(\"test_labels.npy\")\n",
    "train_label = np.load(\"train_labels.npy\")\n",
    "\n",
    "\n",
    "### Choosing Inferences Combinations ###\n",
    "white_list=np.array([4,8,9,10])\n",
    "black_list=np.array([1,5,6,7])\n",
    "gray_list=np.array([0,2,3])\n",
    "#####################################\n",
    "\n",
    "num_classes = np.unique(train_label).shape[0] \n",
    "\n",
    "num_train, height, width = train_data.shape\n",
    "num_test = test_data.shape[0] \n",
    "     \n",
    "white_train_data = np.zeros((0, height, width))\n",
    "black_train_data = np.zeros((0, height, width))\n",
    "gray_train_data = np.zeros((0, height, width))\n",
    "\n",
    "white_test_data = np.zeros((0, height, width))\n",
    "black_test_data = np.zeros((0, height, width))\n",
    "gray_test_data = np.zeros((0, height, width))\n",
    "\n",
    "white_train_label = np.zeros((0))\n",
    "black_train_label = np.zeros((0))\n",
    "gray_train_label = np.zeros((0))\n",
    "\n",
    "white_test_label = np.zeros((0))\n",
    "black_test_label = np.zeros((0))\n",
    "gray_test_label = np.zeros((0))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if i in black_list :\n",
    "        print(\"Info: inference #{} is black-listed\".format(i))\n",
    "        black_train_data = np.append(black_train_data , train_data[train_label == i], axis=0)\n",
    "        black_train_label = np.append(black_train_label , train_label[train_label == i], axis=0)\n",
    "        \n",
    "        black_test_data = np.append(black_test_data , test_data[test_label == i], axis=0)\n",
    "        black_test_label = np.append(black_test_label , test_label[test_label == i], axis=0)\n",
    "    \n",
    "    elif i in gray_list: \n",
    "        print(\"Info: inference #{} is gray-listed\".format(i))\n",
    "        gray_train_data = np.append(gray_train_data , train_data[train_label == i], axis=0)\n",
    "        gray_train_label = np.append(gray_train_label , train_label[train_label == i], axis=0)\n",
    "    \n",
    "        gray_test_data = np.append(gray_test_data , test_data[test_label == i], axis=0)\n",
    "        gray_test_label = np.append(gray_test_label , test_label[test_label == i], axis=0)\n",
    "    \n",
    "    elif i in white_list:\n",
    "        print(\"Info: inference #{} is white-listed\".format(i))\n",
    "        white_train_data = np.append(white_train_data , train_data[train_label == i], axis=0)\n",
    "        white_train_label = np.append(white_train_label , train_label[train_label == i], axis=0)\n",
    "        \n",
    "        white_test_data = np.append(white_test_data , test_data[test_label == i], axis=0)\n",
    "        white_test_label = np.append(white_test_label , test_label[test_label == i], axis=0)\n",
    "\n",
    "white_train_data, white_train_label = shuffle(white_train_data, white_train_label, random_state=0)\n",
    "white_test_data, white_test_label = shuffle(white_test_data, white_test_label, random_state=0)\n",
    "\n",
    "black_train_data, black_train_label = shuffle(black_train_data, black_train_label, random_state=0)\n",
    "black_test_data, black_test_label = shuffle(black_test_data, black_test_label, random_state=0)\n",
    "\n",
    "gray_train_data, gray_train_label = shuffle(gray_train_data, gray_train_label, random_state=0)\n",
    "gray_test_data, gray_test_label = shuffle(gray_test_data, gray_test_label, random_state=0)\n",
    "     \n",
    "### Saving Datasets ###\n",
    "np.save(\"data_train_white.npy\", white_train_data)\n",
    "np.save(\"label_train_white.npy\", white_train_label)\n",
    "print(\"Info: Size of white-listed training data is {}\"\n",
    "      .format(len(white_train_label)))\n",
    "\n",
    "np.save(\"data_train_black.npy\", black_train_data)\n",
    "np.save(\"label_train_black.npy\", black_train_label)\n",
    "print(\"Info: Size of black-listed training data is {}\"\n",
    "      .format(len(black_train_label)))\n",
    "\n",
    "np.save(\"data_train_gray.npy\", gray_train_data)\n",
    "np.save(\"label_train_gray.npy\", gray_train_label)\n",
    "print(\"Info: Size of gray-listed training data is {}\"\n",
    "      .format(len(gray_train_label)))\n",
    "\n",
    "np.save(\"data_test_white.npy\", white_test_data)\n",
    "np.save(\"label_test_white.npy\", white_test_label)\n",
    "print(\"Info: Size of white-listed testing data is {}\"\n",
    "      .format(len(white_test_label)))\n",
    "\n",
    "np.save(\"data_test_black.npy\", black_test_data)\n",
    "np.save(\"label_test_black.npy\", black_test_label)\n",
    "print(\"Info: Size of black-listed testing data is {}\"\n",
    "      .format(len(black_test_label)))\n",
    "\n",
    "np.save(\"data_test_gray.npy\", gray_test_data)\n",
    "np.save(\"label_test_gray.npy\", gray_test_label)\n",
    "print(\"Info: Size of wgray-listed testing data is {}\"\n",
    "      .format(len(gray_test_label)))\n",
    "print(\"Info: all lists are saved\")\n",
    "\n",
    "## Saving Lists\n",
    "all_inferences = [[white_list], [black_list], [gray_list]]\n",
    "np.save(\"all_inferences.npy\", all_inferences)\n",
    "print(\"Info: lists of inferences are saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 4: sm4_replacement_autoencoder.py\n",
    "In this modul we define RAE and train it using subsets created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61945 samples, validate on 15487 samples\n",
      "Epoch 1/10\n",
      "61945/61945 [==============================] - 63s 1ms/step - loss: 0.3700 - val_loss: 0.3073\n",
      "Epoch 2/10\n",
      "61945/61945 [==============================] - 63s 1ms/step - loss: 0.3055 - val_loss: 0.2977\n",
      "Epoch 3/10\n",
      "61945/61945 [==============================] - 62s 1ms/step - loss: 0.2958 - val_loss: 0.2918\n",
      "Epoch 4/10\n",
      "61945/61945 [==============================] - 62s 1ms/step - loss: 0.2905 - val_loss: 0.2935\n",
      "Epoch 5/10\n",
      "61945/61945 [==============================] - 62s 1ms/step - loss: 0.2868 - val_loss: 0.2956\n",
      "Epoch 6/10\n",
      "61945/61945 [==============================] - 62s 1ms/step - loss: 0.2838 - val_loss: 0.2844\n",
      "Epoch 7/10\n",
      "61945/61945 [==============================] - 62s 1ms/step - loss: 0.2828 - val_loss: 0.2830\n",
      "Epoch 8/10\n",
      "61945/61945 [==============================] - 62s 1ms/step - loss: 0.2807 - val_loss: 0.2811\n",
      "Epoch 9/10\n",
      "61945/61945 [==============================] - 62s 1ms/step - loss: 0.2798 - val_loss: 0.2852\n",
      "Epoch 10/10\n",
      "61945/61945 [==============================] - 62s 1ms/step - loss: 0.2787 - val_loss: 0.2820\n",
      "Saved model to disk\n",
      "Info: Transformed data are saved\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created in September 2017\n",
    "@author: mmalekzadeh\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "#####################################\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(2017)\n",
    "#####################################\n",
    "batch_size = 32\n",
    "num_of_epochs = 10\n",
    "\n",
    "w_train_data = np.load(\"data_train_white.npy\")\n",
    "w_test_data = np.load(\"data_test_white.npy\")\n",
    "b_train_data = np.load(\"data_train_black.npy\")\n",
    "b_test_data = np.load(\"data_test_black.npy\") \n",
    "g_train_data = np.load(\"data_train_gray.npy\")\n",
    "g_test_data = np.load(\"data_test_gray.npy\") \n",
    "\n",
    "rnd_idx_train = np.random.choice(g_train_data.shape[0], b_train_data.shape[0], replace=False)\n",
    "rnd_idx_test = np.random.choice(g_test_data.shape[0], b_test_data.shape[0], replace=False)\n",
    "\n",
    "b_train_transformed = g_train_data[rnd_idx_train,:]\n",
    "b_test_transformed = g_test_data[rnd_idx_test,:]\n",
    "\n",
    "x_train = np.append(w_train_data, g_train_data, axis=0)\n",
    "x_train = np.append(x_train, b_train_data, axis=0)\n",
    "x_test = np.append(w_test_data, g_test_data, axis=0)\n",
    "x_test = np.append(x_test, b_test_data, axis=0)\n",
    "x_train_transformed = np.append(w_train_data, g_train_data, axis=0)\n",
    "x_train_transformed = np.append(x_train_transformed , b_train_transformed, axis=0)\n",
    "x_test_transformed = np.append(w_test_data, g_test_data, axis=0)\n",
    "x_test_transformed = np.append(x_test_transformed , b_test_transformed, axis=0)\n",
    "\n",
    "resh=np.prod(w_train_data.shape[1:])\n",
    "x_train = x_train.reshape((len(x_train), resh))\n",
    "x_test = x_test.reshape((len(x_test), resh))\n",
    "x_train_transformed = x_train_transformed.reshape((len(x_train_transformed), resh))\n",
    "x_test_transformed = x_test_transformed.reshape((len(x_test_transformed), resh))\n",
    "\n",
    "##### Replacement Autoencoder #######\n",
    "input_img = Input(shape=(resh,))\n",
    "\n",
    "x = Dense(resh, activation='linear')(input_img)\n",
    "\n",
    "encoded = Dense(resh//2, activation='selu')(x)\n",
    "encoded = Dense(resh//8, activation='selu')(encoded)\n",
    "\n",
    "y = Dense(resh//16, activation='selu')(encoded)\n",
    "\n",
    "decoded = Dense(resh//8, activation='selu')(y)\n",
    "decoded = Dense(resh//2, activation='selu')(decoded)\n",
    "\n",
    "z = Dense(resh, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_img, z)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='mse')\n",
    "\n",
    "autoencoder.fit(x_train , x_train_transformed,\n",
    "                epochs= num_of_epochs,\n",
    "                batch_size = batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test_transformed)\n",
    "                )\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = autoencoder.to_json()\n",
    "with open(\"rae_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "\n",
    "autoencoder.save_weights(\"rae_model_weights.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "rw_test_data = w_test_data.reshape((len(w_test_data), resh))\n",
    "rb_test_data = b_test_data.reshape((len(b_test_data), resh))\n",
    "rg_test_data = g_test_data.reshape((len(g_test_data), resh))\n",
    "\n",
    "transformed_w_test_data = autoencoder.predict(rw_test_data)\n",
    "transformed_b_test_data = autoencoder.predict(rb_test_data)\n",
    "transformed_g_test_data = autoencoder.predict(rg_test_data)\n",
    "\n",
    "transformed_w_test_data = transformed_w_test_data.reshape((len(w_test_data), w_test_data.shape[1], w_test_data.shape[2], 1))\n",
    "transformed_b_test_data = transformed_b_test_data.reshape((len(b_test_data), b_test_data.shape[1], b_test_data.shape[2], 1))\n",
    "transformed_g_test_data = transformed_g_test_data.reshape((len(g_test_data), g_test_data.shape[1], g_test_data.shape[2], 1))\n",
    "\n",
    "### Saving Datasets ###\n",
    "np.save(\"transformed_w_test_data.npy\", transformed_w_test_data)\n",
    "np.save(\"transformed_b_test_data.npy\", transformed_b_test_data)\n",
    "np.save(\"transformed_g_test_data.npy\", transformed_g_test_data)\n",
    "print(\"Info: Transformed data are saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modul 5: sm5_evaluation.py\n",
    "Finally, we compare the output of RAE (transformed data) with the original data to see utility-privacy tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "4779/4779 [==============================] - 2s 487us/step\n",
      "3923/3923 [==============================] - 2s 481us/step\n",
      "6785/6785 [==============================] - 3s 502us/step\n",
      "4779/4779 [==============================] - 2s 465us/step\n",
      "3923/3923 [==============================] - 2s 543us/step\n",
      "6785/6785 [==============================] - 3s 470us/step\n",
      "4779/4779 [==============================] - 2s 468us/step\n",
      "3923/3923 [==============================] - 2s 546us/step\n",
      "6785/6785 [==============================] - 3s 460us/step\n",
      "4779/4779 [==============================] - 2s 501us/step\n",
      "3923/3923 [==============================] - 2s 488us/step\n",
      "6785/6785 [==============================] - 3s 461us/step\n",
      "\n",
      " ~~~ Result: F1-Socre on Original Data:\n",
      "\n",
      " on white-listed: 96.98%\n",
      "\n",
      " on black-listed 93.60%\n",
      "\n",
      " on gray-listed 94.63%\n",
      "\n",
      " ~~~ Result: F1-Socre on Transformed Data:\n",
      "\n",
      " on white-listed: 94.25%\n",
      "\n",
      " on black-listed 0.51%\n",
      "\n",
      " on gray-listed 92.28%\n",
      "Normalized confusion matrix\n",
      "[[0.965 0.001 0.034]\n",
      " [0.002 0.97  0.028]\n",
      " [0.018 0.012 0.97 ]]\n",
      "Normalized confusion matrix\n",
      "[[1.300e-02 2.549e-04 9.867e-01]\n",
      " [4.813e-03 9.464e-01 4.875e-02]\n",
      " [6.780e-03 1.238e-02 9.808e-01]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created in September  2017\n",
    "@author: mmalekzadeh\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "def mcor(y_true, y_pred):\n",
    "     #matthews_correlation\n",
    "     y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "     y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "     y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "     y_neg = 1 - y_pos\n",
    "\n",
    "     tp = K.sum(y_pos * y_pred_pos)\n",
    "     tn = K.sum(y_neg * y_pred_neg)\n",
    " \n",
    "     fp = K.sum(y_neg * y_pred_pos)\n",
    "     fn = K.sum(y_pos * y_pred_neg)\n",
    " \n",
    "     numerator = (tp * tn - fp * fn)\n",
    "     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    " \n",
    "     return numerator / (denominator + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "### Global Variables ###\n",
    "## Load Lists of Inferences\n",
    "all_inferences = np.load(\"all_inferences.npy\")\n",
    "white_list = np.asarray(all_inferences[0].tolist()[0])\n",
    "black_list = np.asarray(all_inferences[1].tolist()[0])\n",
    "gray_list = np.asarray(all_inferences[2].tolist()[0])\n",
    "num_classes = len(white_list)+len(black_list)+len(gray_list)\n",
    "## Load Original Test Data of each list\n",
    "## w -> white, b -> black, g -> gray\n",
    "o_w_test_data = np.load(\"data_test_white.npy\")\n",
    "o_b_test_data = np.load(\"data_test_black.npy\")\n",
    "o_g_test_data = np.load(\"data_test_gray.npy\")\n",
    "o_w_test_data = np.reshape(o_w_test_data,\n",
    "                           (len(o_w_test_data),\n",
    "                            o_w_test_data.shape[1],\n",
    "                            o_w_test_data.shape[2], 1))\n",
    "o_b_test_data = np.reshape(o_b_test_data,\n",
    "                           (len(o_b_test_data),\n",
    "                            o_b_test_data.shape[1],\n",
    "                            o_b_test_data.shape[2], 1))\n",
    "o_g_test_data = np.reshape(o_g_test_data,\n",
    "                           (len(o_g_test_data),\n",
    "                            o_g_test_data.shape[1],\n",
    "                            o_g_test_data.shape[2], 1))\n",
    "## Load Transformed Test Data of each list\n",
    "tr_w_test_data = np.load(\"transformed_w_test_data.npy\")\n",
    "tr_b_test_data = np.load(\"transformed_b_test_data.npy\")\n",
    "tr_g_test_data = np.load(\"transformed_g_test_data.npy\")\n",
    "\n",
    "## Load Labels of each list\n",
    "w_test_label = np.load(\"label_test_white.npy\")\n",
    "b_test_label = np.load(\"label_test_black.npy\")\n",
    "g_test_label = np.load(\"label_test_gray.npy\")\n",
    "\n",
    "\n",
    "## Build one-hot codes for each list\n",
    "y_white = np.zeros((w_test_label.shape[0], num_classes))\n",
    "for i in range(w_test_label.shape[0]):\n",
    "    y_white[i, int(w_test_label[i])] = 1\n",
    "\n",
    "y_black = np.zeros((b_test_label.shape[0], num_classes))\n",
    "for i in range(b_test_label.shape[0]):\n",
    "    y_black[i, int(b_test_label[i])] = 1\n",
    "\n",
    "y_gray = np.zeros((g_test_label.shape[0], num_classes))\n",
    "for i in range(g_test_label.shape[0]):\n",
    "    y_gray[i, int(g_test_label[i])] = 1\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('server_cnn_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"server_cnn_model_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy',precision ,recall])\n",
    "\n",
    "## Evaluate Original Data          \n",
    "o_w_scores = loaded_model.evaluate(o_w_test_data, y_white, verbose=1)  \n",
    "o_b_scores = loaded_model.evaluate(o_b_test_data, y_black, verbose=1) \n",
    "o_g_scores = loaded_model.evaluate(o_g_test_data, y_gray, verbose=1)  \n",
    "## Predict Original Data\n",
    "o_w_predict = loaded_model.predict(o_w_test_data, verbose=1)\n",
    "o_b_predict = loaded_model.predict(o_b_test_data, verbose=1)\n",
    "o_g_predict = loaded_model.predict(o_g_test_data, verbose=1)\n",
    "\n",
    "## Evaluate Transformed Data\n",
    "tr_w_scores = loaded_model.evaluate(tr_w_test_data, y_white, verbose=1)  \n",
    "tr_b_scores = loaded_model.evaluate(tr_b_test_data, y_black, verbose=1) \n",
    "tr_g_scores = loaded_model.evaluate(tr_g_test_data, y_gray, verbose=1)  \n",
    "## Predict Transformed Data\n",
    "tr_w_predict = loaded_model.predict(tr_w_test_data, verbose=1)\n",
    "tr_b_predict = loaded_model.predict(tr_b_test_data, verbose=1)\n",
    "tr_g_predict = loaded_model.predict(tr_g_test_data, verbose=1)\n",
    "\n",
    "print(\"\\n ~~~ Result: F1-Socre on Original Data:\")\n",
    "print(\"\\n on white-listed: %.2f%%\"%\n",
    "      ((2*((o_w_scores[2]*o_w_scores[3])/(o_w_scores[2]+o_w_scores[3])))*100))\n",
    "print(\"\\n on black-listed %.2f%%\"%\n",
    "      ((2*((o_b_scores[2]*o_b_scores[3])/(o_b_scores[2]+o_b_scores[3])))*100))\n",
    "print(\"\\n on gray-listed %.2f%%\"%\n",
    "      ((2*((o_g_scores[2]*o_g_scores[3])/(o_g_scores[2]+o_g_scores[3])))*100))\n",
    "\n",
    "print(\"\\n ~~~ Result: F1-Socre on Transformed Data:\")\n",
    "print(\"\\n on white-listed: %.2f%%\"%\n",
    "      ((2*((tr_w_scores[2]*tr_w_scores[3])/(tr_w_scores[2]+tr_w_scores[3])))*100))\n",
    "print(\"\\n on black-listed %.2f%%\"%\n",
    "      ((2*((tr_b_scores[2]*tr_b_scores[3])/(tr_b_scores[2]+tr_b_scores[3])))*100))\n",
    "print(\"\\n on gray-listed %.2f%%\"%\n",
    "      ((2*((tr_g_scores[2]*tr_g_scores[3])/(tr_g_scores[2]+tr_g_scores[3])))*100))\n",
    "\n",
    "\n",
    "#\n",
    "########### Calculating Confusion Matrix ###########\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 fontsize=18,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "o_w_pred = np.argmax(o_w_predict,axis=1)\n",
    "o_b_pred = np.argmax(o_b_predict,axis=1)\n",
    "o_g_pred = np.argmax(o_g_predict,axis=1)\n",
    "tr_w_pred = np.argmax(tr_w_predict,axis=1)\n",
    "tr_b_pred = np.argmax(tr_b_predict,axis=1)\n",
    "tr_g_pred = np.argmax(tr_g_predict,axis=1)\n",
    "\n",
    "w_true = np.zeros(o_w_pred.shape[0])\n",
    "b_true = np.zeros(o_b_pred.shape[0])\n",
    "g_true = np.zeros(o_g_pred.shape[0])\n",
    "\n",
    "for i in range(o_w_pred.shape[0]):\n",
    "    w_true[i]= 1\n",
    "    if o_w_pred[i] in gray_list:\n",
    "        o_w_pred[i]= 2\n",
    "    elif o_w_pred[i] in white_list:\n",
    "        o_w_pred[i]= 1\n",
    "    else:\n",
    "        o_w_pred[i]= 0\n",
    "\n",
    "for i in range(o_b_pred.shape[0]):\n",
    "    b_true[i]= 0\n",
    "    if o_b_pred[i] in gray_list:\n",
    "        o_b_pred[i]= 2\n",
    "    elif o_b_pred[i] in white_list:\n",
    "        o_b_pred[i]= 1\n",
    "    else:\n",
    "        o_b_pred[i]= 0\n",
    "\n",
    "for i in range(o_g_pred.shape[0]):\n",
    "    g_true[i]= 2\n",
    "    if o_g_pred[i] in gray_list:\n",
    "        o_g_pred[i]= 2\n",
    "    elif o_g_pred[i] in white_list:\n",
    "        o_g_pred[i]= 1\n",
    "    else:\n",
    "        o_g_pred[i]= 0\n",
    "\n",
    "\n",
    "for i in range(tr_w_pred.shape[0]):\n",
    "    if tr_w_pred[i] in gray_list:\n",
    "        tr_w_pred[i]= 2\n",
    "    elif tr_w_pred[i] in white_list:\n",
    "        tr_w_pred[i]= 1\n",
    "    else:\n",
    "        tr_w_pred[i]= 0\n",
    "\n",
    "for i in range(tr_b_pred.shape[0]):\n",
    "    if tr_b_pred[i] in gray_list:\n",
    "        tr_b_pred[i]= 2\n",
    "    elif tr_b_pred[i] in white_list:\n",
    "        tr_b_pred[i]= 1\n",
    "    else:\n",
    "        tr_b_pred[i]= 0\n",
    "\n",
    "for i in range(tr_g_pred.shape[0]):\n",
    "    if tr_g_pred[i] in gray_list:\n",
    "        tr_g_pred[i]= 2\n",
    "    elif tr_g_pred[i] in white_list:\n",
    "        tr_g_pred[i]= 1\n",
    "    else:\n",
    "        tr_g_pred[i]= 0\n",
    "\n",
    "\n",
    "class_names =[\"B\", \"W\", \"G\"]\n",
    "ycf_test = np.append(w_true, g_true, axis=0)\n",
    "ycf_test = np.append(ycf_test, b_true, axis=0)\n",
    "ycf_o_pred = np.append(o_w_pred, o_g_pred, axis=0)\n",
    "ycf_o_pred = np.append(ycf_o_pred, o_b_pred, axis=0)\n",
    "ycf_tr_pred = np.append(tr_w_pred, tr_g_pred, axis=0)\n",
    "ycf_tr_pred = np.append(ycf_tr_pred, tr_b_pred, axis=0)\n",
    "\n",
    "## Compute confusion matrix for Original Data\n",
    "o_cnf_matrix = confusion_matrix(ycf_test, ycf_o_pred)\n",
    "np.set_printoptions(precision=3)\n",
    "## Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(o_cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Confusion Matrix of Original Data')\n",
    "plt.savefig('OCF.pdf',bbox_inches='tight')\n",
    "\n",
    "plt.gcf().clear()\n",
    "\n",
    "## Compute confusion matrix for Transformed Data\n",
    "tr_cnf_matrix = confusion_matrix(ycf_test, ycf_tr_pred)\n",
    "np.set_printoptions(precision=3)\n",
    "## Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(tr_cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Confusion Matrix of Transformed Data')\n",
    "plt.savefig('TrCF.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
